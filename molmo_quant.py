# -*- coding: utf-8 -*-
"""Molmo-Quant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nrHl__K-lECZRTYYoaOYhQQeHKSN8ROY

# Mount Drive and Install Dependencies
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -U transformers

!pip install einops transformers accelerate bitsandbytes

"""# Load Preprocessor and VLM"""

from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig
from PIL import Image
import requests
import torch

# Can also be a local path if you have already cloned the hugging face repo
#MODEL_PATH = "allenai/Molmo-7B-D-0924"
#MODEL_PATH = "/content/drive/MyDrive/Molmo_Quant/Molmo_7B_D_Quant"
#MODEL_PATH = "allenai/MolmoE-1B-0924"
MODEL_PATH = "reubk/MolmoE-1B-0924-NF4"

# load the processor
processor = AutoProcessor.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    device_map='auto'
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    device_map='auto'
)

model

"""# Testing VLM on single or series of image/text prompts"""

with open("example.txt", "w") as file:
    file.write("Hello, World!\n")
    file.write("This is a new line.")

# process the image and text
for i in range(1,48):
  filename = f"/content/trainshot ({i}).png"
  image = Image.open(filename).convert("RGB")
  # Open the image and convert to RGB to handle potential alpha channel issues with PNGs
  inputs = processor.process(
      images=[image],
      text="You are operating a rover for disaster relief operation. Point to the humans (dead or alive) visible in the camera."
  )

  # move inputs to the correct device and make a batch of size 1
  inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}

  # Compute is done in float16, while most weights are NF4
  with torch.autocast(device_type="cuda", enabled=True, dtype=torch.float16):
      output = model.generate_from_batch(
          inputs,
          GenerationConfig(max_new_tokens=400, stop_strings="<|endoftext|>"),
          tokenizer=processor.tokenizer,
          use_cache=False
      )

  # only get generated tokens; decode them to text
  generated_tokens = output[0, inputs['input_ids'].size(1):]
  generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)

  # print the generated text
  print(filename+'|'+generated_text+'\n')
  with open("eval.txt", "w") as file:
    file.write(filename+'|'+generated_text+'\n')

filename="https://as1.ftcdn.net/v2/jpg/02/18/02/14/1000_F_218021446_KGQscsIXv2pYSAFSDC6gzdX3yN546gKH.jpg"
# Open the image from URL and convert to RGB
import requests
from io import BytesIO

r = requests.get(filename, headers={"User-Agent": "python-requests"}, timeout=30)
r.raise_for_status()
image = Image.open(BytesIO(r.content)).convert("RGB")

inputs = processor.process(
    images=[Image.open(BytesIO(r.content)).convert("RGB")],
    text="Now that you have verified that this is the missing person, output the code to move the motor in the direction"
)

# move inputs to the correct device and make a batch of size 1
inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}



inputs['image_input_idx'].shape

# Compute is done in float16, while most weights are NF4
with torch.autocast(device_type="cuda", enabled=True, dtype=torch.float16):
    output = model.generate_from_batch(
        inputs,
        GenerationConfig(max_new_tokens=200, stop_strings="<|endoftext|>"),
        tokenizer=processor.tokenizer,
        use_cache=False
    )

# only get generated tokens; decode them to text
generated_tokens = output[0, inputs['input_ids'].size(1):]
generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)

# print the generated text
print(generated_text+'\n')

processor.tokenizer.decode(output[0])

output.size()

generated_tokens.size()

inputs['input_ids'].size(1)

output.shape

inputs['input_ids'].size(1)

# @title Default title text
generated_text = "<points x1=\"40.5\" y1=\"66.2\" x2=\"42.7\" y2=\"66.2\" x3=\"43.6\" y3=\"77.6\" x4=\"44.0\" y4=\"66.2\" x5=\"52.6\" y5=\"66.2\" x6=\"55.2\" y6=\"66.2\" x7=\"57.9\" y7=\"65.9\" alt=\"humans (dead or alive) visible in the camera\">humans (dead or alive) visible in the camera</points>" # @param {"type":"string"}
filepath = "/content/shot (39).png" # @param {"type":"string"}
width = 1920 # @param {"type":"integer"}
height = 1080 # @param {"type":"integer"}
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
from PIL import Image

try:
  html = generated_text
  soup = BeautifulSoup(html, 'html.parser')
  tag = soup.find('point')

  if tag:
    coordsx = []
    coordsy = []
    coordsx.append(float(tag[f'x']) / 100 * width)
    coordsy.append(float(tag[f'y']) / 100 * height)
    label = tag['alt']
  else:
    tag = soup.find('points')
    # Extract values
    coordsx = []
    coordsy = []
    for i in range(1, 16):
        coordsx.append(float(tag[f'x{i}']) / 100 * width)
        coordsy.append(float(tag[f'y{i}']) / 100 * height)
    label = tag['alt']
except:
  print('')

#open image
img = Image.open(filepath)
# Plot
fig, ax = plt.subplots(figsize=(16, 9)) # Increase figure size
ax.imshow(img)
ax.scatter(coordsx, coordsy)

plt.axis('off')
plt.show()
print(generated_text)

with open("/content/eval_1bmoe_lora_16.txt", "r") as f:
  lines = f.readlines()

from PIL import Image

inputs = processor.process(
    images=[Image.open("/content/OIP (3).jpg")],
    text="Describe this image please."
  )

inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}

# generate output; maximum 200 new tokens; stop generation when <|endoftext|> is generated
output = model.generate(
    **inputs,
    generation_config=GenerationConfig(max_new_tokens=200, stop_strings="<|endoftext|>"),
    tokenizer=processor.tokenizer
)

"""#Quantization Code for Molmo"""

from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# Can also be a local path if you have already cloned the hugginface repo
# MODEL_PATH = "allenai/Molmo-7B-D-0924"
MODEL_PATH = "allenai/MolmoE-1B-0924"
YOUR_OUTPUT_PATH = "/content/MolmoE-1B-0924-NF4"

DEFAULT_DTYPE = torch.float16

nf4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=DEFAULT_DTYPE,
    llm_int8_skip_modules=[
        # Module names can also be relative like "ff_norm" which would apply to all such layers
        "model.vision_backbone", "model.transformer.ff_out", "model.transformer.ln_f"
    ]
)

# load the model
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    device_map='auto',
    torch_dtype=DEFAULT_DTYPE,
    quantization_config=nf4_config,
)

model.config

import json

# Convert the config object to a dictionary and then print it
print(json.dumps(model.config.to_dict(), indent=2))

import torch
# Save raw state dict directly
torch.save(model.state_dict(), "/content/drive/MyDrive/Molmo_Quant/MolmoE-1B-0924-NF4/pytorch_model.bin")

model.model.transformer

# Save model
model.save_pretrained(
    save_directory=YOUR_OUTPUT_PATH,
    safe_serialization=True,
    # Set a maximum shard size if you don't like the default
    max_shard_size="4GB"
)

from huggingface_hub import HfApi
api = HfApi()

# Upload all the content from the local folder to your remote Space.
# By default, files are uploaded at the root of the repo
api.upload_folder(
    folder_path=YOUR_OUTPUT_PATH,
    repo_id="reubk/MolmoE-1B-0924-NF4",
    repo_type="model"
)

"""#LoRA fine-tuning code for Molmo"""

!pip install peft pandas matplotlib seaborn trl

!pip install datasets

import os
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
os.environ['TORCH_USE_CUDA_DSA'] = "1"

from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig
from PIL import Image
import requests
import torch

# Can also be a local path if you have already cloned the hugging face repo
#MODEL_PATH = "allenai/Molmo-7B-D-0924"
#MODEL_PATH = "/content/drive/MyDrive/Molmo_Quant/Molmo_7B_D_Quant"
#MODEL_PATH = "allenai/MolmoE-1B-0924"
MODEL_PATH = "reubk/MolmoE-1B-0924-NF4"

# load the processor
processor = AutoProcessor.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    device_map='auto',
    use_fast=True
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    device_map='auto'
)

model.tie_weights()

import pandas as pd

# @title Custom Molmo Data Code

import torch
import pandas as pd
import json
import numpy as np
from PIL import Image
from torch.utils.data import Dataset
from typing import Dict, List, Any, Optional
import os

class MolmoCountingDataset(Dataset):
    """
    Custom dataset class for Molmo multimodal counting task fine-tuning.

    Expected CSV format:
    - image_url: path to image file
    - points: JSON string with coordinate points
    - count: number of objects
    - label: object type (e.g., 'sock')
    - collection_method: method used (e.g., 'counting')
    - response: target response with special formatting
    """

    def __init__(
        self,
        csv_path: str,
        processor,
        image_base_path: str = "",
        max_length: int = 1536,
        prompt_template: Optional[str] = None
    ):
        """
        Args:
            csv_path: Path to the CSV file
            processor: MolmoProcessor instance
            image_base_path: Base path to prepend to image URLs if they're relative
            max_length: Maximum sequence length
            prompt_template: Template for creating prompts. If None, uses default.
        """
        self.df = pd.read_csv(csv_path)
        self.processor = processor
        self.image_base_path = image_base_path
        self.max_length = max_length

        # Image token IDs as provided by the user
        self.image_tokens = [152064, 152065, 152066, 152067, 152068]

        # Default prompt template that creates a conversation
        if prompt_template is None:
            self.prompt_template = "Count the number of {label} in this image."
        else:
            self.prompt_template = prompt_template

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        row = self.df.iloc[idx]

        # Load image
        image_path = os.path.join(self.image_base_path, row['image_url'])
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"Error loading image {image_path}: {e}")
            # Return a blank white image as fallback
            image = Image.new('RGB', (336, 336), color='white')

        # Create prompt from the template
        prompt = self.prompt_template.format(
            label=row['label'],
            count=row['count'],
            points=row['points']
        )

        # Get target response
        target_response = row['response']

        # Process the user prompt to get the input tokens up to "Assistant:"
        try:
            processed_input = self.processor.process(
                text=prompt,
                images=image,
                text_kwargs={
                    "sequence_length": self.max_length,
                    "padding": False,
                    "message_format": "role",  # This adds "User: ... Assistant:" format
                    "always_start_with_space": True
                }
            )
        except Exception as e:
            print(f"Error processing input for item {idx}: {e}")
            # Return a minimal processed item as fallback
            processed_input = self.processor.process(
                text="Count objects in this image.",
                images=None,
                text_kwargs={
                    "sequence_length": self.max_length,
                    "padding": False,
                    "message_format": "role"
                }
            )

        # Now manually add the assistant response tokens
        # First, tokenize the target response without special formatting
        target_tokens = self.processor.tokenizer.encode(
            target_response,
            add_special_tokens=False
        )

        # Get the input sequence (which ends with "Assistant:")
        input_ids = processed_input["input_ids"]

        # Concatenate input + target response tokens
        full_input_ids = torch.cat([
            input_ids,
            torch.tensor(target_tokens, dtype=torch.long)
        ])

        # Truncate if too long
        if len(full_input_ids) > self.max_length:
            full_input_ids = full_input_ids[:self.max_length]

        # Create labels for causal language modeling
        # We want to predict the assistant's response, so mask the user prompt
        labels = full_input_ids.clone()

        # Find where to start predicting (after the user prompt)
        prompt_length = len(input_ids)

        # Mask the user prompt tokens (set to -100 to ignore in loss)
        labels[:prompt_length] = -100

        result = {
            "input_ids": full_input_ids.squeeze(0),
            "labels": labels.squeeze(0),
            "target_response": target_response,
            "original_prompt": prompt,
        }

        # Add image-related data if present and ensure correct dtype
        if "images" in processed_input and processed_input["images"] is not None:
            result["images"] = processed_input["images"].to(torch.float32)
            result["image_input_idx"] = processed_input["image_input_idx"]
            if "image_masks" in processed_input:
                result["image_masks"] = processed_input["image_masks"].to(torch.float32)

        return result


def molmo_sft_collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
    """
    Specialized collate function for SFT training that better handles response masking.
    This version ensures only the assistant's response contributes to the loss.
    """
    batch_size = len(batch)
    max_seq_len = max(len(item["input_ids"]) for item in batch)

    # Initialize tensors
    input_ids = torch.zeros((batch_size, max_seq_len), dtype=torch.long)
    labels = torch.full((batch_size, max_seq_len), -100, dtype=torch.long)  # -100 = ignore
    attention_mask = torch.zeros((batch_size, max_seq_len), dtype=torch.long)

    # Handle images
    has_images = "images" in batch[0] and batch[0]["images"] is not None
    if has_images:
        # Find the maximum number of image patches across the batch
        max_image_patches = max(item["images"].shape[0] if "images" in item and item["images"] is not None else 0 for item in batch)

        if max_image_patches > 0:
            image_shape = batch[0]["images"].shape[1:]

            # Initialize images tensor with an appropriate padding value (e.g., -1.0 for image data)
            images = torch.full((batch_size, max_image_patches, *image_shape), -1.0, dtype=torch.float32)

            # Find the maximum tokens per image patch index across the batch
            max_tokens_per_image_patch = max(item["image_input_idx"].shape[1] if "image_input_idx" in item else 0 for item in batch)

            image_input_idx = torch.full(
                (batch_size, max_image_patches, max_tokens_per_image_patch),
                -1, dtype=torch.long
            )

            image_masks = None
            if "image_masks" in batch[0]:
                image_masks = torch.zeros((batch_size, max_image_patches, image_shape[0]), dtype=torch.float32)


    # Fill batch data
    for i, item in enumerate(batch):
        seq_len = len(item["input_ids"])

        input_ids[i, :seq_len] = item["input_ids"]
        labels[i, :seq_len] = item["labels"]
        attention_mask[i, :seq_len] = 1

        if has_images and "images" in item and item["images"] is not None:
            num_crops = item["images"].shape[0]
            images[i, :num_crops] = item["images"]

            num_image_patches = item["image_input_idx"].shape[0]
            image_input_idx[i, :num_image_patches, :item["image_input_idx"].shape[1]] = item["image_input_idx"]

            if image_masks is not None and "image_masks" in item:
                image_masks[i, :num_crops] = item["image_masks"]

    result = {
        "input_ids": input_ids,
        "labels": labels,
        "attention_mask": attention_mask,
    }

    if has_images and max_image_patches > 0:
        result["images"] = images
        result["image_input_idx"] = image_input_idx
        if image_masks is not None:
            result["image_masks"] = image_masks

    return result


def debug_tokenization(dataset, idx=0):
    """
    Helper function to debug tokenization and label creation.
    """
    item = dataset[idx]
    processor = dataset.processor

    print(f"Original prompt: {item['original_prompt']}")
    print(f"Target response: {item['target_response']}")
    print(f"Input IDs shape: {item['input_ids'].shape}")
    print(f"Labels shape: {item['labels'].shape}")

    # Decode the input_ids to see what tokens we have
    decoded_input = processor.tokenizer.decode(item['input_ids'], skip_special_tokens=False)
    print(f"Decoded input: {decoded_input}")

    # Show which parts will contribute to loss (labels != -100)
    non_ignore_labels = item['labels'][item['labels'] != -100]
    if len(non_ignore_labels) > 0:
        decoded_labels = processor.tokenizer.decode(non_ignore_labels, skip_special_tokens=False)
        print(f"Tokens for loss (non-masked): {decoded_labels}")
    else:
        print("No tokens for loss computation (all labels are masked)")

    # Check for image tokens
    image_token_count = sum(1 for token_id in item['input_ids'] if token_id in dataset.image_tokens)
    print(f"Number of image tokens: {image_token_count}")

    if "images" in item:
        print(f"Images shape: {item['images'].shape}")
        print(f"Image input idx shape: {item['image_input_idx'].shape}")

dataset = MolmoCountingDataset(
    csv_path="/content/trainshot.csv",
    processor=processor,
    image_base_path="",
    prompt_template="You are operating a rover for disaster relief operation. Point to the humans (dead or alive) visible in the camera."
)

for i in range(47):
  print(dataset[i]['images'].dtype)

from transformers import TrainingArguments

# Define training arguments
training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/molmoe_pointing_finetuned",  # Output directory
    num_train_epochs=20,  # Number of training epochs
    per_device_train_batch_size=4,  # Batch size per device during training
    learning_rate=2e-5,  # Learning rate
    logging_dir="./logs",  # Directory for storing logs
    logging_steps=3, # Log every 100 steps
    save_steps=25, # Save checkpoint every 25 steps
    save_total_limit=2, # Limit the total number of checkpoints
    bf16=True,  # Use bfloat16 precision
    tf32=True,  # Use tf32 precision
)

#for Molmo 7B
attention_modules = []
"""
for i in range(0, 23):
    attention_modules.extend([
        f"vision_backbone.image_vit.transformer.resblocks.{i}.attention.wq",
        f"vision_backbone.image_vit.transformer.resblocks.{i}.attention.wk",
        f"vision_backbone.image_vit.transformer.resblocks.{i}.attention.wv",
        f"vision_backbone.image_vit.transformer.resblocks.{i}.attention.wo",
        f"vision_backbone.image_vit.transformer.resblocks.{i}.feed_forward.w1",
        f"vision_backbone.image_vit.transformer.resblocks.{i}.feed_forward.w2",
    ])
"""
attention_modules.extend([
        "vision_backbone.image_pooling_2d.wq",
        "vision_backbone.image_pooling_2d.wk",
        "vision_backbone.image_pooling_2d.wv",
        "vision_backbone.image_pooling_2d.wo"
    ])

attention_modules.extend([
        "vision_backbone.image_projector.w1",
        "vision_backbone.image_projector.w2",
        "vision_backbone.image_projector.w3"
    ])
for i in range(0, 23):
    attention_modules.extend([
        f"transformer.blocks.{i}.attn_out",
        f"transformer.blocks.{i}.att_proj",
        #f"transformer.blocks.{i}.ff_out",
        #f"transformer.blocks.{i}.ff_proj"
    ])

experts = [62, 17, 52, 2, 51, 27, 6, 54, 7, 60] #, 35, 1, 39]

#for MolmoE 1B
attention_modules = []

attention_modules.extend([
        "vision_backbone.image_pooling_2d.wq",
        "vision_backbone.image_pooling_2d.wk",
        "vision_backbone.image_pooling_2d.wv",
        "vision_backbone.image_pooling_2d.wo"
    ])

attention_modules.extend([
        "vision_backbone.image_projector.w1",
        "vision_backbone.image_projector.w2",
        "vision_backbone.image_projector.w3"
    ])
for i in range(0, 16):
    attention_modules.extend([f"transformer.blocks.{i}.mlp.gate"])
    for j in experts:
        attention_modules.extend([
          f"transformer.blocks.{i}.mlp.experts.{j}.gate_proj",
          f"transformer.blocks.{i}.mlp.experts.{j}.up_proj",
          f"transformer.blocks.{i}.mlp.experts.{j}.down_proj",
          #f"transformer.blocks.{i}.ff_out",
          #f"transformer.blocks.{i}.ff_proj"
      ])

from peft import LoraConfig, get_peft_model

# Define LoRA configuration
loraconfig = LoraConfig(
    r=16, # LoRA attention dimension
    lora_alpha=16, # Alpha parameter for LoRA
    # Target the linear layers in the vision backbone
    target_modules=attention_modules,
    lora_dropout=0.05, # Dropout probability for LoRA layers
    bias="none", # Bias type for LoRA layers
    task_type="CAUSAL_LM", # Task type
)

# Apply LoRA to model first
model = get_peft_model(model, loraconfig)
model.print_trainable_parameters()

model

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,  # Your original dataset with __getitem__ returning input_ids
    data_collator=molmo_sft_collate_fn,  # Your custom collate function
)

trainer.args=training_args

outputs = model(input_ids=dataset[4]['input_ids'].unsqueeze(0), labels=dataset[4]['labels'].unsqueeze(0))
loss = outputs.loss
loss.backward()

model

import os
os.environ['CUDA_LAUNCH_BLOCKING'] ='1'
os.environ['TORCH_USE_CUDA_DSA'] ='1'

!set CUDA_LAUNCH_BLOCKING = 1
!set TORCH_USE_CUDA_DSA=1

dataset[5]

# Start training
trainer.train()

# Manually save the model's state dictionary and processor after training
# This is a workaround for the JSON serialization error during trainer.save_model()
try:
    # Attempt to save the state dictionary which includes LoRA weights
    torch.save(model.state_dict(), os.path.join(training_args.output_dir, "pytorch_model.bin"))
    print("Model state dictionary saved successfully.")
except Exception as e:
    print(f"Error saving model state dictionary: {e}")

try:
    # Save the processor
    processor.save_pretrained(training_args.output_dir)
    print("Processor saved successfully.")
except Exception as e:
    print(f"Error saving processor: {e}")

# You will need to create a test dataset similar to the training dataset
# Replace 'None' with your actual test dataset
test_dataset = dataset # Replace with your test dataset

if test_dataset is not None:
    eval_results = trainer.evaluate(eval_dataset=test_dataset)
    print(eval_results)
else:
    print("No test dataset provided for evaluation.")

# Save the fine-tuned model
trainer.save_model("./molmo_pointing_finetuned")

model

from trl import SFTConfig
args = SFTConfig(
    output_dir="/content/drive/MyDrive/molmo_pointing_finetuned", # directory to save and repository id
    num_train_epochs=15,                     # number of training epochs
    per_device_train_batch_size=4,          # batch size per device during training
    gradient_accumulation_steps=8,          # number of steps before performing a backward/update pass
    gradient_checkpointing=False,            # use gradient checkpointing to save memory
    optim="adamw_torch_fused",              # use fused adamw optimizer
    logging_steps=3,                       # log every 10 steps
    save_strategy="epoch",                  # save checkpoint every epoch
    learning_rate=2e-5,                     # learning rate, based on QLoRA paper
    #bf16=True,                              # use bfloat16 precision
    #tf32=True,                              # use tf32 precision
    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper
    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper
    lr_scheduler_type="constant",           # use constant learning rate scheduler
    push_to_hub=True,                       # push model to hub
    report_to="tensorboard",                # report metrics to tensorboard
    #gradient_checkpointing_kwargs = {"use_reentrant": False}, # use reentrant checkpointing
    dataset_text_field="", # need a dummy field for collator
    dataset_kwargs = {"skip_prepare_dataset": True} # important for collator
)

from trl import SFTTrainer

trainer = SFTTrainer(
    model=model,
    args=args,
    train_dataset=dataset,
    data_collator=molmo_sft_collate_fn,
    peft_config=loraconfig
)

# @title Fix?
import torch
import pandas as pd
import json
import numpy as np
from PIL import Image
from torch.utils.data import Dataset
from typing import Dict, List, Any, Optional
import os
class MolmoCountingDatasetForSFT(Dataset):
    def __init__(self, csv_path: str, processor, image_base_path: str = "", prompt_template: Optional[str] = None):
        self.df = pd.read_csv(csv_path)
        self.processor = processor
        self.image_base_path = image_base_path

        if prompt_template is None:
            self.prompt_template = "Count the number of {label} in this image."
        else:
            self.prompt_template = prompt_template

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        row = self.df.iloc[idx]

        # Load image
        image_path = os.path.join(self.image_base_path, row['image_url'])
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"Error loading image {image_path}: {e}")
            image = Image.new('RGB', (336, 336), color='white')

        # Create the conversation text
        user_prompt = self.prompt_template.format(label=row['label'])
        target_response = row['response']

        # Format as conversation
        conversation_text = f"User: {user_prompt}\nAssistant: {target_response}"

        return {
            "text": conversation_text,  # SFTTrainer expects this key
            "images": image
        }

processor.tokenizer.decode(dataset[3]['input_ids'], skip_special_tokens=True)

processor.tokenizer.decode([21388], skip_special_tokens=True)

i=986
print(f"ID: {dataset[0]['input_ids'][i]} Label: {dataset[0]['labels'][i]}")

from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig
from peft import PeftModel
from PIL import Image
import torch

# Load the fine-tuned LoRA model
lora_model_path = "/content/drive/MyDrive/molmoe_pointing_finetuned/checkpoint-175"
model = PeftModel.from_pretrained(model, lora_model_path)

model

# Open the image and convert to RGB to handle potential alpha channel issues with PNGs
inputs = processor.process(
    images=[Image.open("/content/Screenshot 2025-10-07 175801.png").convert("RGB"),Image.open("/content/Screenshot 2025-10-13 224311.png").convert("RGB")],
    text="Are there two people wearing the same attire? just say yes or no and explain in one sentence"
)

# move inputs to the correct device and make a batch of size 1
inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}

# Compute is done in float16, while most weights are NF4
with torch.autocast(device_type="cuda", enabled=True, dtype=torch.float16):
    output = model.generate_from_batch(
        inputs,
        GenerationConfig(max_new_tokens=200, stop_strings="<|endoftext|>"),
        tokenizer=processor.tokenizer,
        use_cache=False
    )

# only get generated tokens; decode them to text
generated_tokens = output[0, inputs['input_ids'].size(1):]
generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)

# print the generated text
print(filename+'|'+generated_text+'\n')

"""# Moondream testing

"""

from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image
import matplotlib.pyplot as plt

model = AutoModelForCausalLM.from_pretrained(
    "vikhyatk/moondream2",
    revision="2025-06-21",
    trust_remote_code=True,
    device_map={"": "cuda"}  # ...or 'mps', on Apple Silicon
)

for i in range(1,45):
  image =  Image.open(f'/content/shot ({i}).png').convert("RGB")
  points = model.point(image, "person")["points"]
  width = 1920
  height = 1080
  x_coords = [point['x'] * width for point in points]
  y_coords = [point['y'] * height for point in points]
  fig, ax = plt.subplots(figsize=(16, 9)) # Increase figure size
  ax.imshow(image)
  ax.scatter(x_coords, y_coords)

"""# MolmoActs"""

!pip install einops torchvision accelerate
!pip install transformers==4.52

from transformers import AutoProcessor, AutoModelForImageTextToText
import torch
from PIL import Image
import requests
from io import BytesIO

ckpt = "allenai/MolmoAct-7B-D-0812"

# load the processor
processor = AutoProcessor.from_pretrained(
    ckpt,
    trust_remote_code=True,
    torch_dtype="auto",
    device_map="auto",
    padding_side="left",
)

# load the model
model = AutoModelForImageTextToText.from_pretrained(
    ckpt,
    trust_remote_code=True,
    torch_dtype="auto",
    device_map="auto",
)

# task instruction
instruction = "to navigate the route"

# strictly follow this reasoning prompt
prompt = (
    f"The task is {instruction}. "
    "What is the action that the robot should take. "
    f"To figure out the action that the robot should take to {instruction}, "
    "let's think through it step by step. "
    "First, what is the depth map for the first image? "
    "Second, what is the trajectory of the end effector in the first image? "
    "Based on the depth map of the first image and the trajectory of the end effector in the first image, "
    "along with other images from different camera views as additional information, "
    "what is the action that the robot should take?"
)

# apply chat template
text = processor.apply_chat_template(
    [
        {
            "role": "user",
            "content": [dict(type="text", text=prompt)]
        }
    ],
    tokenize=False,
    add_generation_prompt=True,
)

# image observation (side + wrist)
#url1 = "https://huggingface.co/allenai/MolmoAct-7B-D-0812/resolve/main/example_1.png"
#url2 = "https://huggingface.co/allenai/MolmoAct-7B-D-0812/resolve/main/example_2.png"
#r1 = requests.get(url1, headers={"User-Agent": "python-requests"}, timeout=30)
#r1.raise_for_status()
#r2 = requests.get(url2, headers={"User-Agent": "python-requests"}, timeout=30)
#r2.raise_for_status()
img1 = Image.open('/content/photo_6141001530845807348_y.jpg').convert("RGB")
#img2 = Image.open(BytesIO(r2.content)).convert("RGB")
imgs = [img1]#, img2]

# process the image and text
inputs = processor(
    images=[imgs],
    text=text,
    padding=True,
    return_tensors="pt",
)

# move inputs to the correct device
inputs = {k: v.to(model.device) for k, v in inputs.items()}

# generate output
with torch.inference_mode():
    with torch.autocast("cuda", enabled=True, dtype=torch.bfloat16):
        generated_ids = model.generate(**inputs, max_new_tokens=256)

# only get generated tokens; decode them to text
generated_tokens = generated_ids[:, inputs['input_ids'].size(1):]
generated_text = processor.batch_decode(generated_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]

# print the generated text
print(f"generated text: {generated_text}")

# >>>  The depth map of the first image is ... The trajectory of the end effector in the first image is ...
#      Based on these information, along with other images from different camera views as additional information,
#      the action that the robot should take is ...

# parse out all depth perception tokens
depth = model.parse_depth(generated_text)
print(f"generated depth perception tokens: {depth}")

# >>>  [ "<DEPTH_START><DEPTH_1><DEPTH_2>...<DEPTH_END>" ]

# parse out all visual reasoning traces
trace = model.parse_trace(generated_text)
print(f"generated visual reasoning trace: {trace}")

# >>>  [ [[242, 115], [140, 77], [94, 58], [140, 44], [153, 26]]] ]

# parse out all actions, unnormalizing with key of "molmoact"
action = model.parse_action(generated_text, unnorm_key="molmoact")
print(f"generated action: {action}")

import matplotlib.pyplot as plt
from PIL import Image

# The generated visual reasoning trace coordinates
trace_coords = [[[153, 204], [145, 119], [133, 119], [119, 113], [130, 123]]]#[[[177, 102], [179, 97], [207, 5], [246, 14], [231, 17]]]
# >>>  [[242, 115], [140, 77], [94, 58], [140, 44], [153, 26]]]

# Assuming you want to plot the trace on the first image used in the previous cell

# You might need to change the image path if you want to use a different image
#img_url = "https://huggingface.co/allenai/MolmoAct-7B-D-0812/resolve/main/example_1.png"
#r = requests.get(img_url, headers={"User-Agent": "python-requests"}, timeout=30)
#r.raise_for_status()
#img = Image.open(BytesIO(r.content)).convert("RGB")

# Extract x and y coordinates from the trace
coordsx = [point[0] for point in trace_coords[0]]
coordsy = [point[1] for point in trace_coords[0]]

# Plot
img1=img1.resize((320,240))
fig, ax = plt.subplots()
ax.imshow(img1)
ax.plot(coordsx, coordsy)

plt.axis('off')
plt.show()

"""# RAG Multimodal

"""

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel, AutoImageProcessor
from PIL import Image
import requests

processor = AutoImageProcessor.from_pretrained("nomic-ai/nomic-embed-vision-v1.5")
vision_model = AutoModel.from_pretrained("nomic-ai/nomic-embed-vision-v1.5", trust_remote_code=True)

#url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
#image = Image.open(requests.get(url, stream=True).raw)

images = []
for i in range(1,279):
  filename = f"/content/frame{i}.jpg"
  # Open the image and convert to RGB to handle potential alpha channel issues with PNGs
  images.append(Image.open(filename).convert("RGB"))


inputs = processor(images, return_tensors="pt")

img_emb = vision_model(**inputs).last_hidden_state
img_embeddings = F.normalize(img_emb[:, 0], p=2, dim=1)

def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

sentences = ['search_query: mirror' ,]

tokenizer = AutoTokenizer.from_pretrained('nomic-ai/nomic-embed-text-v1.5')
text_model = AutoModel.from_pretrained('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True)
text_model.eval()

encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')

with torch.no_grad():
    model_output = text_model(**encoded_input)

text_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
text_embeddings = F.layer_norm(text_embeddings, normalized_shape=(text_embeddings.shape[1],))
text_embeddings = F.normalize(text_embeddings, p=2, dim=1)

print(torch.matmul(img_embeddings, text_embeddings.T))

text_embeddings.shape

img_embeddings.shape

# Compute cosine similarity matrix
similarity_matrix = F.cosine_similarity(img_embeddings.unsqueeze(1), img_embeddings.unsqueeze(0), dim=2)

# Display the shape of the similarity matrix to confirm dimensions
print(similarity_matrix)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Flatten the similarity matrix and remove the diagonal (self-similarity)
# We use the upper triangle excluding the diagonal to avoid duplicates and self-similarity
upper_triangle_flat = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)].detach().cpu().numpy()

# Plot histogram of similarity scores
plt.figure(figsize=(10, 6))
sns.histplot(upper_triangle_flat, bins=50, kde=True)
plt.title('Distribution of Cosine Similarity Scores (Excluding Self-Similarity)')
plt.xlabel('Cosine Similarity')
plt.ylabel('Frequency')
plt.show()

# Print descriptive statistics
print("Descriptive statistics of cosine similarity scores (excluding self-similarity):")
print(pd.Series(upper_triangle_flat).describe())

# Choose a threshold based on the distribution (e.g., slightly above the mean or median)
# Let's start with a value based on the histogram shape and descriptive stats
# A common approach is to look for a natural break or a point where the frequency starts to drop significantly.
# Based on a typical distribution, a threshold around 0.6 or 0.7 might be reasonable, but we'll check the stats.
# Let's pick a value slightly higher than the mean as a starting point for the threshold.
similarity_threshold = float(pd.Series(upper_triangle_flat).quantile(0.99)) # Example: Using the 90th percentile as a starting point
print(f"\nChosen similarity threshold (90th percentile): {similarity_threshold}")

# Initialize an empty dictionary to represent the navigational tree
navigational_tree = {}

# Build the navigational tree based on KNN results and chronological neighbors
num_images = img_embeddings_np.shape[0]
k_neighbors = 3 # Number of nearest neighbors to extract from KNN (excluding self)

# Re-run KNN with the specified k
nn_model = NearestNeighbors(n_neighbors=k_neighbors + 1, metric='cosine') # k+1 to include self
nn_model.fit(img_embeddings_np)
distances, indices = nn_model.kneighbors(img_embeddings_np)


for i in range(num_images):
    # Initialize an empty list for each image's neighbors
    navigational_tree[i] = []

    # Add KNN neighbors (excluding self)
    # indices[i, 1:] contains the indices of the k nearest neighbors
    navigational_tree[i].extend(indices[i, 1:].tolist())

    # Add chronological connections
    # Add connection to the next image if it exists
    if i + 1 < num_images:
        if (i + 1) not in navigational_tree[i]:
            navigational_tree[i].append(i + 1)

    # Add connection to the previous image if it exists
    if i - 1 >= 0:
        if (i - 1) not in navigational_tree[i]:
            navigational_tree[i].append(i - 1)

    # Ensure unique neighbors and sort the list
    navigational_tree[i] = sorted(list(set(navigational_tree[i])))


# Display the first few entries of the navigational tree to verify
print("Navigational Tree (KNN + Chronological - first 5 entries):")
for i in range(min(5, num_images)):
    print(f"Image {i}: {navigational_tree[i]}")

# 1. Initialize an empty dictionary to represent the navigational tree
navigational_tree = {}

# 2. Iterate through the similarity_matrix and add similarity-based connections
num_images = similarity_matrix.shape[0]
for i in range(num_images):
    navigational_tree[i] = []  # Initialize an empty list for each image

for i in range(num_images):
    for j in range(i + 1, num_images): # Iterate through the upper triangle (j > i)
        # a. Check if the cosine similarity is greater than or equal to the threshold
        if similarity_matrix[i, j] >= similarity_threshold:
            # b. Add a link between image i and image j
            navigational_tree[i].append(j)
            navigational_tree[j].append(i) # Add the symmetric link

# 3. Add chronological connections
for i in range(num_images):
    # Add connection to the next image if it exists
    if i + 1 < num_images:
        if (i + 1) not in navigational_tree[i]:
            navigational_tree[i].append(i + 1)
        if i not in navigational_tree[i + 1]:
            navigational_tree[i + 1].append(i)

    # Add connection to the previous image if it exists
    if i - 1 >= 0:
        if (i - 1) not in navigational_tree[i]:
            navigational_tree[i].append(i - 1)
        if i not in navigational_tree[i - 1]:
            navigational_tree[i - 1].append(i)

# Sort the neighbor lists for consistency (optional)
for i in range(num_images):
    navigational_tree[i].sort()

# Display the first few entries of the navigational tree to verify
print("Navigational Tree (first 5 entries):")
for i in range(min(5, num_images)):
    print(f"Image {i}: {navigational_tree[i]}")

# Explore connections for a few sample images
print("\nExploring connections for sample images:")
#sample_images = [0, 10, 25, 50, 77] # Select a few diverse image indices

for img_index in range(num_images):
    if img_index < num_images:
        neighbors = navigational_tree[img_index]
        print(f"Image {img_index} is connected to: {neighbors}")
    else:
        print(f"Image {img_index} is out of bounds (total images: {num_images}).")

import networkx as nx
import matplotlib.pyplot as plt

# Create a graph from the navigational tree dictionary
G = nx.Graph(navigational_tree)

# Visualize the graph
plt.figure(figsize=(12, 12))
pos = nx.spring_layout(G, seed=42)  # positions for all nodes - seed for reproducibility
nx.draw(G, pos, with_labels=True, node_size=500, node_color='skyblue', font_size=10, edge_color='gray')
plt.title("Navigational Tree of Similar Images")
plt.show()

import matplotlib.pyplot as plt
from PIL import Image

# Specify the paths to the two images you want to display
image_path1 = "/content/frame232.jpg"  # Replace with the path to your first image
image_path2 = "/content/frame276.jpg"  # Replace with the path to your second image

# Open the images
img1 = Image.open(image_path1).convert("RGB")
img2 = Image.open(image_path2).convert("RGB")

# Create a figure with two subplots (1 row, 2 columns)
fig, axes = plt.subplots(1, 2, figsize=(10, 5))

# Display the first image in the first subplot
axes[0].imshow(img1)
axes[0].set_title("Image 1")
axes[0].axis('off')  # Hide axes

# Display the second image in the second subplot
axes[1].imshow(img2)
axes[1].set_title("Image 2")
axes[1].axis('off')  # Hide axes

# Adjust layout to prevent titles from overlapping
plt.tight_layout()

# Show the plot
plt.show()